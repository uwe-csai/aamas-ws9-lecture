{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9c1bef",
   "metadata": {},
   "source": [
    "# Model-free Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaeb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Match, Plots\n",
    "using Distributions, StatsBase, RollingFunctions, Random\n",
    "include(\"operators.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63615fba",
   "metadata": {},
   "source": [
    "# Definition of the environment (MDP)\n",
    "\n",
    "State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [if i==j && i==2 () else (i,j) end for i=1:3,j=1:4 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa879ea",
   "metadata": {},
   "source": [
    "For convenience can also be expressed as a unidimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caeada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sₗ = reshape(S, 12,1)\n",
    "Sₗ = [ s for s in Sₗ if s != ()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263352a9",
   "metadata": {},
   "source": [
    "Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [\"↑\",\"↓\",\"←\",\"→\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5cd56",
   "metadata": {},
   "source": [
    "Function for checking that actions are valid within our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1daa1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OK(xy) = if xy[1] > 0 && xy[2] > 0 && xy[1] <= size(S,1) && xy[2] <= size(S,2) && S[xy[1],xy[2]] != ()  true else false end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9365baa",
   "metadata": {},
   "source": [
    "We also need to know which states are terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2038b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_terminal(s) = if s == (1,4) || s == (2,4) true else false end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800642b",
   "metadata": {},
   "source": [
    "Function returning the next state if we take a given action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "Af((i,j),a) =  @match a begin\n",
    "                \n",
    "    \"↑\" =>  if OK((i-1,j)) \n",
    "                (i-1,j) \n",
    "            else (i,j) end\n",
    "    \"↓\" =>  if OK((i+1,j))\n",
    "                (i+1,j) \n",
    "            else (i,j) end\n",
    "    \"←\" => if OK((i,j-1)) \n",
    "                (i,j-1) \n",
    "            else (i,j)  end\n",
    "    \"→\" => if OK((i,j+1))\n",
    "            (i,j+1)\n",
    "        else (i,j)  end\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2472c46",
   "metadata": {},
   "source": [
    "Note as this is a model-free method **the agent does not know the transition probabilities**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af29cd",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0590930",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [@match (i,j) begin (2,2)=> nothing\n",
    "                        (1,4) => 1. \n",
    "                        (2,4) => -1. \n",
    "                        _ => 0.\n",
    "                    end for i=1:3,j=1:4 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea1aae",
   "metadata": {},
   "source": [
    "The reward for a particular state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r(s) = R[s[1],last(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482a9fc",
   "metadata": {},
   "source": [
    "The discount factor ($\\gamma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b376f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.9\n",
    "\n",
    "Γ(s) = @match s begin\n",
    "            (1,4) => 0.0 #Terminal state\n",
    "            (2,4) => 0.0 #Terminal state\n",
    "            _ => γ\n",
    "        end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752f9c1",
   "metadata": {},
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "## Q-values initialisation\n",
    "\n",
    "Initialise arbitrary to 0 for every action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q₀() = begin\n",
    "    Q₀v = repeat([(\"\",0.)],3,4,4) # size of State space and Action space\n",
    "    [ Q₀v[i,j,k] = (A[k],0.0) for k=1:4,j=1:4 ,i=1:3  ]\n",
    "    return Q₀v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b2f19",
   "metadata": {},
   "source": [
    "## Create a policy to use in the learning process\n",
    "\n",
    "### We will use a $\\epsilon$*-greedy* policy\n",
    "\n",
    "* With a probability $\\epsilon$ the agent takes any action (random)\n",
    "* With a probability $( 1 - \\epsilon)$ the agent takes the action with max $Q$*-value*\n",
    "\n",
    "\n",
    "We need to define the probabilities of each action in our policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ab68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pϵ(Q,s) = begin\n",
    "    x,y = s\n",
    "    Aₛ = Q[x,y,:]\n",
    "    Aₛ = sort(Aₛ,by=last,rev=true) #sort estimated Q value-action\n",
    "    \n",
    "    #get the index of the action with max Q-value\n",
    "    a⁺ᵢ = first(indexin([first(first(Aₛ))],A))\n",
    "\n",
    "    # Fill out probabilities according to ϵ-greedy\n",
    "    P = repeat([ϵ/(length(A) - 1)],length(A))\n",
    "    P[a⁺ᵢ] = 1 - ϵ\n",
    "\n",
    "    return P\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67001ef",
   "metadata": {},
   "source": [
    "Define $\\epsilon$ and $\\alpha$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 0.05\n",
    "α = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f867f",
   "metadata": {},
   "source": [
    "Now we define the policy that uses the defined probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "πϵ(Q,s) = begin\n",
    "    Pₛ = Pϵ(Q,s)\n",
    "    a = sample(A,Weights(Pₛ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d4da1",
   "metadata": {},
   "source": [
    "### main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6084d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning(n) = begin\n",
    "\n",
    "    # Initialise Q arbitrary   \n",
    "    Qᵥ = Q₀()\n",
    "    \n",
    "    #reset our environment \n",
    "    s₀ = (3,1) \n",
    "    #s₀ = sample(Sₗ) \n",
    "\n",
    "\n",
    "    # For storing stats\n",
    "    lgs = [] # length of episodes\n",
    "    rtns = [] # return of episodes\n",
    "\n",
    "    # For n episodes\n",
    "\n",
    "    for episode = 1:n\n",
    "        # restart the agent in initial position\n",
    "        s = s₀ \n",
    "        \n",
    "        #Accumulator of rewards for the episode\n",
    "        rtn = 0 #episode return\n",
    "        \n",
    "        # Arbitrary number of steps to try\n",
    "        for step = 1:100\n",
    "            \n",
    "            x,y = s\n",
    "            \n",
    "            # Get the action to take from our ϵ-greedy policy\n",
    "            a = πϵ(Qᵥ,s)\n",
    "            # index of the action\n",
    "            aᵢ = first(indexin([a],A))\n",
    "\n",
    "            #For current state calculates:\n",
    "            \n",
    "            # next state\n",
    "            s′ = Af(s,a)\n",
    "            x′,y′ = s′\n",
    "\n",
    "            # reward\n",
    "            rₛ = r(s)\n",
    "            # accumulate episode return\n",
    "            rtn = rtn + rₛ\n",
    "            \n",
    "            # maxₐ Q(S′,A)\n",
    "            A′ = Qᵥ[x′,y′,:]\n",
    "            a′ = first(sort(A′,by=last,rev=true))           \n",
    "            a′ᵢ = first(indexin([first(a′)],A)) \n",
    "\n",
    "            # Temporal difference target: R + γ maxₐ Q(S′,A)\n",
    "            td⁺ = rₛ + Γ(s) * last(Qᵥ[x′,y′,a′ᵢ])\n",
    "            \n",
    "            # Temporal difference: R + γ maxₐ Q(S′,a) - Q(S,A)\n",
    "            tdΔ = td⁺ - last(Qᵥ[x,y,aᵢ])\n",
    "\n",
    "            # Q(S,A) = Q(S,A) + α [R + γ maxₐ Q(S′,a) - Q(S,A)]\n",
    "            newQvalue = last(Qᵥ[x,y,aᵢ]) + α * tdΔ\n",
    "\n",
    "            # update Qᵥ array\n",
    "            newQᵥ = copy(Qᵥ)\n",
    "            newQᵥ[x,y,aᵢ] = (\"$(A[aᵢ])\", newQvalue)\n",
    "            Qᵥ = newQᵥ\n",
    "\n",
    "            #if in terminal state stores stats and break steps loop\n",
    "            if is_terminal(s)\n",
    "                push!(lgs,step)\n",
    "                push!(rtns,rtn)\n",
    "                break\n",
    "            end\n",
    "            # takes step \n",
    "            s = s′\n",
    "        end\n",
    "    end\n",
    "   return Qᵥ,lgs,rtns\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabe34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qᵥ,lgts,rtns = q_learning(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a1f1e",
   "metadata": {},
   "source": [
    "Convergence:\n",
    "\n",
    "The resulting policy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ \n",
    "    @match (i,j) begin\n",
    "        (2,2)   => ()\n",
    "        (1,4) => \"\"\n",
    "        (2,4) => \"\"\n",
    "        _ => first(first(sort(Qᵥ[i,j,:],by=last,rev=true)))\n",
    "    end\n",
    "    for i=1:3,j=1:4 \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5facf6c",
   "metadata": {},
   "source": [
    "Calculate reward convergence with a moving average (window size = 10 ep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgts_rm = rollmean(lgts * 1.,10)\n",
    "rtns_rm = rollmean(rtns * 1.,10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plot(rtns_rm ./ lgts_rm),xlabel=\"episode\",ylabel=\"mean (Reward / Step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1293a",
   "metadata": {},
   "source": [
    "Number of steps required in each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c95538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(lgts_rm,xlabel=\"episodes\",ylabel=\"steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
